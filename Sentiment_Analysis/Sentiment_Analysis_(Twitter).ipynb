{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y9z3MdVXiSE",
        "outputId": "5776c546-cdd4-4db3-c2a4-a11ef18222e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh3fNSvvXjX8",
        "outputId": "5961c367-3c9e-41ca-e0ef-9fed678c3faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
            "License(s): other\n",
            "Downloading sentiment140.zip to /content\n",
            " 90% 73.0M/80.9M [00:00<00:00, 129MB/s]\n",
            "100% 80.9M/80.9M [00:00<00:00, 118MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT-rRpz8X2um",
        "outputId": "e6bc9095-734a-475d-fd04-f09d5b743e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  sentiment140.zip\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip sentiment140.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAtO_GKJXukU",
        "outputId": "279ce9aa-abaa-47cd-fe48-c0eb83136220"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-5-724d5402b6c0>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['target'] = df['target'].map({0: 'negative', 4: 'positive'})\n",
            "<ipython-input-5-724d5402b6c0>:29: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")  # Remove HTML tags\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     target                                               text\n",
            "0  negative  switchfoot awww thats bummer shoulda got david...\n",
            "1  negative  upset cant update facebook texting might cry r...\n",
            "2  negative  kenichan dived many time ball managed save res...\n",
            "3  negative                    whole body feel itchy like fire\n",
            "4  negative           nationwideclass behaving im mad cant see\n"
          ]
        }
      ],
      "source": [
        "# Load necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load dataset (Twitter Sentiment Analysis)\n",
        "df = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',\n",
        "                 encoding='ISO-8859-1',\n",
        "                 names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
        "\n",
        "# Filter out the columns you need\n",
        "df = df[['target', 'text']]\n",
        "\n",
        "# Map target values: 0 for negative, 4 for positive (convert to 0, 1)\n",
        "df['target'] = df['target'].map({0: 'negative', 4: 'positive'})\n",
        "\n",
        "# Clean the text data (same as original cleaning process)\n",
        "stop = set(stopwords.words('english'))\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")  # Remove HTML tags\n",
        "    text = soup.get_text()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()])\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Check the cleaned data\n",
        "print(df.head())\n",
        "\n",
        "# Proceed with data analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO4_xwdbXxNc"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels\n",
        "label_encode = LabelEncoder()\n",
        "y_data = label_encode.fit_transform(df['target'])\n",
        "\n",
        "# Split data into train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['text'], y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "x_train_encoded = tfidf_vectorizer.fit_transform(x_train)\n",
        "x_test_encoded = tfidf_vectorizer.transform(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9DnuztdYreP",
        "outputId": "82bf661c-a695-4ed9-f98b-ad06e3a869bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree Accuracy: 0.714921875\n"
          ]
        }
      ],
      "source": [
        "# Train Decision Tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "dt_classifier.fit(x_train_encoded, y_train)\n",
        "y_pred = dt_classifier.predict(x_test_encoded)\n",
        "print(f'Decision Tree Accuracy: {accuracy_score(y_pred, y_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(x_train_encoded, y_train)\n",
        "y_pred_rf = rf_classifier.predict(x_test_encoded)\n",
        "print(f'Random Forest Accuracy: {accuracy_score(y_pred_rf, y_test)}')"
      ],
      "metadata": {
        "id": "fFRt1e-iHbbX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}